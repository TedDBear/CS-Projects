b'A*-tree: A Structure for Storage and Modeling of Uncertain \nMultidimensional Arrays \nTingjian Ge \nUniversity of Kentucky \nge@cs.uky.edu Stan Zdonik \nBrown University \nsbz@cs.brown.edu\n \nAbstract  \nMultidimensional array database sy stems are suited for scientific \nand engineering applications. Data in these applications is often \nuncertain and imprecise due to errors in the instruments and \nobservations, etc. There are often correlations exhibited in the distribution of values among the ce lls of an array. Typically, the \ncorrelation is stronger for cells that are close to each other and weaker for cells that are far away. We devise a novel data structure, called the A*-tree  (multi dimensional Array tree), \ndemonstrating that by taking advantage of the predictable and structured correlations of multidimensional data, we can have a more efficient way of modeling and answering queries on large-\nscale array data. An A*-tree is a unified model for storage and inference. The graphical model that is assumed in an A*-tree is essentially a Bayesian Network. We analyze and experimentally verify the accuracy of an A*-tree encoding of the underlying joint distribution. We also study the efficiency of query processing \nover A*-trees, comparing it to an alternative graphical model. \n1. INTRODUCTION \nMultidimensional array database systems are suited for \nscientific and engineering appli cations. Several array database \nsystems have been designed and im plemented, such as T2 [7], \nTitan [8], RasDaMan [2], ArrayDB [19], ASAP [24], and the most recent one, SciDB [27]. In this data model, each cell of a \nmultidimensional array is a tuple  (potentially multiple attributes, \ne.g., temperature, humidity, etc.). If we consider each cell\xe2\x80\x99s array index at each dimension as an additional attribute of the cell tuple, a multi-dimensional array is logically equivalent to a (one-dimensional) relational table. That is to say, a multidimensional array is logically equivalent to a table with schema ( A\n1, A2, \xe2\x80\xa6, Ak \n, D1, D2, \xe2\x80\xa6, Dd), where each cell originally only has k attributes \nA1 to A k , and D1 to D d are the d dimensions of the array. Thus, an \narray system still follows a relational model. \nData in these applications is often uncertain and imprecise due \nto errors in the instruments and observations, etc. There are often correlations in the distributions of an uncertain attribute among \ncells of an array. For example, a temperature attribute can be correlated with other array cells (one random variable per cell). \nBy the nature of the data that arrays model, the correlations are generally stronger for cells that ar e close to each other and weaker \notherwise. Consider the following example. Example 1.  Environment monitoring in an open field often \nproduces sensor readings over time. However, resource constraints (e.g., sensor power and network capability) often prevent sensors from sending readings at every point in time. At a given time, we may only have an outdated reading for each sensor in the network. The actual curre nt reading is from a distribution \nin a range around the outdated one [9]. All sensors (which can be \neither real sensors or  interpolated readings [15]) in the network \nform a two-dimensional array with  the dimensions being sensor \nlocation. The uncertain attribut e is correlated with neighboring \ncells. \nWe provide two additional examples in Appendix A. In many \nsuch applications, the representati on of the uncertain data needs to \nencode the value correlation among tuples for the result to be correct. Ignoring the correlation and making an over-simplified tuple independence assumption of ten renders the query results \nwrong and useless. We illustrate th is in the experiment section. \nHowever, modeling attribute co rrelation among cell tuples is \nnot an easy task, simply due to the large number of cells in many arrays and the arbitrary correlati ons among them. In this work, we \nargue that by taking advantage of predictable and structured correlations of multidimensional data, we can provide a more efficient way of modeling and answering queries on large-scale array data. We propose a new data structure, called the A*-tree  \n(multi dimensional Array tree). The A*-tree approach is based on \nthe following observation: data in a multidimensional array is usually correlated along some dime nsions and the correlation is \nlargely local. Thus, if we have to sacrifice precision by allowing approximate models, we should focus on local correlation. An \nA*-tree uses this fact and organize data in a hierarchical manner. Within the hierarchical structure, the joint distributions are smaller and can be modeled more efficiently. \nThere is a simple mapping from the graph structure  of an A*-\ntree (i.e., the storage model of an array) to its probabilistic \ngraphical model. We show that the graphical model of an A*-tree is a Bayesian Network (BN). Physically, only the leaves of the tree-structured BN exist. The nodes (i.e., random variables) at \nupper levels are derived  from the leaves. Thus, the construction of \nan A*-tree is bottom-up, yet the probabilistic inference (which is \nneeded for processing queries [22, 25]) is top-down. \nBecause A*-tree integrates both the probabilistic graphical \nmodel and the physical storage model, probabilistic inference is very efficient. It requires traversing the A*-tree and following a logarithmic-length path directly to the needed cells of the array. We study query processing techniques for both general queries and specifically COUNT, AVG, and SUM queries which permit optimizations using A*-trees. We also compare A*-trees with an alternative graphical model. To summarize, the paper\xe2\x80\x99s contributions are: \n\xef\x82\xb7 We propose a novel data structure,  A*-tree, suited to modeling \nthe correlation in a multidimensional array (Sec. 2). \nPermission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.  Articles from this volume were presented at The 36th International Conference on Very Large Data Bases, September 13-17, 2010, Singapore. \nProceedings of th e VLDB Endowment, Vol. 3, No. 1 \n\xc2\xa9 2010 VLDB Endowment  2150-8097/10/09... $10.00 \n964'
b' Figure 1.  Example of a k-ary tree with k = 4 \xef\x82\xb7 We analyze how A*-trees balance the accuracy of modeling \ncorrelation and the efficiency of query processing (Sec. 3). \n\xef\x82\xb7 We study query processing techniques, demonstrating the advantage of efficient inference with A*-trees (Sec. 4). \n\xef\x82\xb7 We conduct a systematic experime ntal study on a real dataset \nas well as a synthetic dataset (Sec. 5). \n2. A*-TREE STRUCTURE \nIn this section, we describe the A*-tree structure and how it \nencodes the joint distribution of array cells. \n \n \n   \n2.1 Basic A*-tree Structure \nAn A*-tree is a k-ary tree [11] (e.g., Figure 1, where dotted \nbranches indicate which children are missing) with the degree k \nbeing 2d, where d is the number of dimensions in which the \nuncertain value is correlated. Note that d is typically small (most \noften 1, 2, or 3). Thus, it is a binary tree when d = 1 and a 4-ary \ntree when d = 2, and so on. Figure 2 shows an example partition \nfor d = 2. Throughout this section, we use d = 2. This can be \neasily extended to other dimensionalities. Similar to quadtrees \n[16],  we recursively divide an array in half along each dimension. \nIn Figure 2, the first partition (thick dotted lines) divides the array space into four ( k = 2\n2) subspaces. The whole array maps to the \nroot of the 4-ary tree in Figure 1, and the four subspaces map to its four children in some fixed order (e.g., 1\nst child is the north-\nwest subspace, 2nd child is the south-west one, etc.). Then \nrecursively, we again partition each  of the four subspaces into \nfour, which map to the four children of each node at the level below the root in Figure 1. Thus, a recursive partition of the array space corresponds to a top-down traversal of the k-ary A*-tree \nfrom one level to the next. Eventually, at the leaf level, each leaf corresponds to four neighboring cell values of the array. In Figure 2, array cells A, B, C, and D  together form a leaf. \n \n    \n \n \nFor now, for simplicity of exposition, in the case of d > 1, we \nassume that each of the dimensions has the same size. We also \nassume this size is 2\nn (for some integer n). It is easy to generalize \nit to an arbitrary size (as shown in Appendix D.1). The black blocks in Figure 2 indicate the empty  regions (NULL values) of \nthe array that do not have values in the A*-tree and, thus, correspond to \xe2\x80\x9cmissing\xe2\x80\x9d children in this 4-ary A*-tree. Thus, arrays of arbitrary sparsity can be accommodated.  Here is how a joint distribution is encoded in an A*-tree: \n\xef\x82\xb7 Each leaf in an A*-tree contains the joint distribution of four \nneighboring cells of the array. This joint distribution is a conditional one, conditioned on the four cell\xe2\x80\x99s average value. In the example in Fig. 2, ther e are four cell random variables \nA, B, C, and D. Define a random variable X = (A+B+C+D)/4. \nThen there is a leaf in the A*-tree that contains the joint distribution of A , B, C and D conditioned on X. \n\xef\x82\xb7 Recursively, in a bottom-up manner, an internal node of the A*-tree encodes the joint distribution of its four children conditioned on their average. A child node is a random variable that is the average of all cells of the array covered under its subtree. \n\xef\x82\xb7 In addition to this joint distribution, the root node also holds the distribution of the average value of the whole array. \nNote that the average of children is weighted . For example, in \nthe A*-tree of Figure 1, node N contains the joint distribution of \nits three children (one child node is missing), A, B, and C, \nconditioned on their average value ( n\nA\xc2\xb7A+n B\xc2\xb7B+n C\xc2\xb7C) / \n(nA+nB+nC), where nA is the number of non-empty cells (i.e., not \nNULL) in the subtree rooted at A; similarly for nB and nC. \nThe key idea of A*-trees is that we model the joint distribution \nof cells in a manageable way that is relatively compact and automatically structured. The automatic structure is based on the principle of the locality of data correlation: closer cells are more likely correlated. We organize cells into hierarchical clusters according to proximity, each of which contains a small number of random variables so that we can encode their joint distribution compactly. \nAn interesting aspect of the A*-tree approach is that if we \nsimply trim the leaves of an A*-tree, the remaining A*-tree represents the distribution of an array with a coarser grain. This enables a fast approximation of the data and may be meaningful for many applications that dema nd rapid results (e.g., real-time \nprocessing or on-line computation) . For the example of image and \nsound, object or pattern recognition algorithms can work in the coarser level. In a real-time network system, quick decisions at a higher level can be crucial for meeting real-time constraints. \nFinally, we note that A*-trees partition the space in an array \nsimilar to the way quadtrees [16] do (for two dimensions).  Our main contribution in this work , however, is about using this \npartition scheme to succinctly model the correlations and joint distribution of the array cells.  We  show that this is a natural \nunification of both the storage model and the probabilistic graphical model (Section 3). \n2.2 Extensions of the Basic A*-tree Structure \n2.2.1 Basic Uncertainty Blocks of Arbitrary Shapes \nWe define a basic uncertainty block of an array as a box (e.g., a \nrectangle for two-dimensional arra ys) in the array inside which \ncells have the same distribution. In the basic A*-tree of Section \n2.1, each array cell is a basic uncertainty block. This is the smallest basic block size po ssible. However, in many \napplications, this granularity is not necessary and the basic block \nsize can be much larger. Having a larger basic block size makes the representation more succinc t and query processing more \nefficient. \nFor example, astronomers take photo images of objects in the \nuniverse. Due to precision limits, pixels of an image, treated as cells of a two-dimensional array, exhibit correlated uncertainty in \ntheir values. A block of neighboring pixels, due to their proximity, is likely to have the same error distribution. Thus, a basic uncertainty block can be, say, 50 by 50 cells in size. Now each basic block is treated as a \xe2\x80\x9csingle cell\xe2\x80\x9d in an A*-tree, which  \nN \nA B C \nA\nBCDABCD\nFigure 2. Illustrating recursive partit ioning of a two-dimensional array. \nThe joint distribution of the uncertain a ttribute is encoded in a 4-ary tree. \n965'
