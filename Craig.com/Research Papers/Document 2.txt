b'Paci\xef\xac\x81c Graphics 2021\nE. Eisemann, K. Singh, and F.-L Zhang(Guest Editors)V olume 40 (2021), Number 7\nWrite-An-Animation: High-level Text-based Animation Editing\nwith Character-Scene Interaction\nJia-Qi Zhang1, Xiang Xu1, Zhi-Meng Shen1, Ze-Huan Huang1, Yang Zhao1, Yan-Pei Cao2, Pengfei Wan2, Miao Wang1\xe2\x80\xa0\n1State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China\n2Y-tech, Kuaishou Technology\n\xe2\x80\x9cEve walks to the house\xe2\x80\x9d \xe2\x80\x9cIn the room, an organized office \ndesk is near a bed$.\xe2\x80\x9d\n\xe2\x80\x9cEve walks to the couch.\xe2\x80\x9d\xe2\x80\x9cIn the living room, there is a bookcase$. \nA couch is in front of a tv$. \nA chair is near a piano$.\xe2\x80\x9d\n\xe2\x80\x9ccamera front\xe2\x80\x9d \xe2\x80\x9cEve sits on the couch.\nand sings with the microphone\xe2\x80\x9d\nFigure 1: Write-An-Animation interface and animation generation process. After initialization, the user can edit texts in the textbox to\nspecify the virtual scene, character motion, and virtual camera, etc. The dollar sign is added to the end of the script to distinguish scene\ndescriptions from character-motion commands. The 3D animation will be iteratively generated and updated according to the user\xe2\x80\x99s input.\nAbstract3D animation production for storytelling requires essential manual processes of virtual scene composition, character creation,and motion editing, etc. Although professional artists can favorably create 3D animations using software, it remains a complexand challenging task for novice users to handle and learn such tools for content creation. In this paper , we present Write-An-Animation, a 3D animation system that allows novice users to create, edit, preview, and render animations, all through textediting. Based on the input texts describing virtual scenes and human motions in natural languages, our system \xef\xac\x81rst parses thetexts as semantic scene graphs, then retrieves 3D object models for virtual scene composition and motion clips for character\nanimation. Character motion is synthesized with the combination of generative locomotions using neural state machine as well\nas template action motions retrieved from the dataset. Moreover , to make the virtual scene layout compatible with charactermotion, we propose an iterative scene layout and character motion optimization algorithm that jointly considers character-\nobject collision and interaction. We demonstrate the effectiveness of our system with customized texts and public \xef\xac\x81lm scripts.\nExperimental results indicate that our system can generate satisfactory animations from texts.\nCCS Concepts\n\xe2\x80\xa2Computing methodologies \xe2\x86\x92 Motion processing;\n\xe2\x80\xa0Corresponding author: miaow@buaa.edu.cn1. Introduction\nWith the pervasive use of mobile phones and social networks,\ndiverse video media such as vlogs, sel\xef\xac\x81es and animated \xef\xac\x81lms\xc2\xa9 2021 The Author(s)\nComputer Graphics Forum \xc2\xa9 2021 The Eurographics Association and John\nWiley & Sons Ltd. Published by John Wiley & Sons Ltd.DOI: 10.1111/cgf.14415'
b'Zhang et al. / Write-An-Animation: High-level Text-based Animation Editing with Character-Scene Interaction\nhave now become increasingly popular. Compared with tradi-\ntional texts or pictures, an increasing number of people prefer towatch and share animated \xef\xac\x81lms. However, the production of 3D\nanimation or animated \xef\xac\x81lms is complicated, which not only re-\nquires a variety of professional software but also highly relies onuser\xe2\x80\x99s skill and experience. With the development of deep learn-\ning technology in recent years, researchers have proposed various\ncost-effective content generation methods such as language-driven\nsketch colorization [ZMG\n\xe2\x88\x9719], facial animation [AHK\xe2\x88\x9702], video\nmontage [WYH\xe2\x88\x9719] and animation [HID\xe2\x88\x9714]. Among them, text-\ndriven animation has been a hot but challenging research topic.\nAlthough several animation synthesis methods were pro-\nposed [AHKM20, GCO\xe2\x88\x9721], to the best of our knowledge, no prior\nwork has focused on the combination of virtual scene editing with\ncharacter-motion synthesis. Some work was devoted to solving 3Dindoor scene generation [CSM14b, CSM14a], while others aimed\nto improve virtual character motion realism [HKS17, SZKS19]. We\nwould like to argue that, seperate generations of high-quality vir-tual scenes or character motions cannot guarantee the animation\nquality of their combinations. On the one hand, characters can in-\nteract with the virtual scene, on the other hand, scene layout can\nimplicitly affect character\xe2\x80\x99s behavior like locomotion trajectory.\nIn this work, we present Write-An-Animation, a text-based ani-\nmation editing system, where users can edit virtual scene layout,control character locomotion in the scene and specify the inter-\naction with scene models, all through texts. Our system supportsnatural language texts, and allows users to edit texts for scenelayout and character motion editing. The interaction interface of\nWrite-An-Animation consists of a textbox and an execution but-\nton, which is simple yet effective. The user can iteratively edit the\ntext in the textbox and update the animation result (see Figure 1).\nWe clarify that our text-based animation editing is high-level and\nfriendly to novice users, where users do not need to set any param-\neters for character pose, walking trajectory, etc. Nevertheless, userscan futher \xef\xac\x81ne-tune animation details with the help of commercial\nsoftware, which is not the foucs of this paper. Our system is com-\npatible to motion capture (mocap) data, as long as the Biovisionhierarchical data (BVH) and corresponding action tags are indexedto our system. We demonstrate our system using customized texts\nand texts from public \xef\xac\x81lm scripts for single character animation in\nindoor scenes. Experimental results including ablation studies and\nsubjective studies indicate that our system can produce satisfactoryanimation. To summarize, the major contributions of this paper are:\n\xe2\x80\xa2An interactive, high-level text-based 3D animation editing sys-\ntem that drives human character animation in high-quality virtualscene.\n\xe2\x80\xa2An algorithm for scene layout and interactive character motion\noptimization.\n\xe2\x80\xa2A character-motion blending algorithm from generative locomo-\ntion and template action representations. It facilitates the joint\noptimization of scene layout and character motion and is com-patible with new mocap data.\n2. Related Work\nText-based Image and Video Synthesis. Recently, image and\nvideo applications with natural languages as inputs have attractedincreasing attention, due to the advance of natural language pro-\ncessing. Zhu et al. [ZGE\n\xe2\x88\x9707] proposed a text-to-picture synthesis\nsystem to augment the input text with retrieved multiple images.\nWhile their goal was to augment the communication, the gener-ated results were separated images rather than a composed image\nfor storytelling. Built upon neural networks, Fu et al. [ ZMG\n\xe2\x88\x9719]\nproposed a text-based sketch colorization system. With simple text\ninstructions, the input sketch image was colorized step by step. Re-\ncently, Wang et al. [WYH\xe2\x88\x9719] proposed a text-based video mon-\ntage system to retrieve video clips from datasets and compose\nvideos that followed cinematographic guidelines. Text-based edit-ing of talking-head video [ FTZ\n\xe2\x88\x9719] was proposed to produce high-\nquality portrait videos, especially for interview videos. The work\nindicates that utilizing natural languages as inputs is a promising\ndirection for image and video synthesis.\nText-based Scene Generation. 3D scene generation methods\nfrom various input types such as RGB-D images [CLW\xe2\x88\x9714],\nsketches [XCF\xe2\x88\x9713], and languages [MPF\xe2\x88\x9718] have been proposed.\nAmong them, text-based scene synthesis has been a hot topic.\nChang et al. [CSM14b, CSM14a] transformed texts into graphs, and\nthen extracted the spatial knowledge of objects, such as occurrence,hierarchy, surface and relative position, to generate a reasonablelayout. Chang et al. [CMS\n\xe2\x88\x9715] further extended this work by in-\ntroducing a dataset with natural language descriptions and learningmappings from data to texts. Rui et al. [ MPF\n\xe2\x88\x9718] described ob-\njects in scenes as semantic scene graphs to retrieve scenes in the\ndataset that best matched the text. Although this approach depends\non semantic scene graph construction for the dataset, it can ensure\nreasonable scene layout, which is convenient to add or delete mod-els in scenes. For more work on scene synthesis, please refer to thesurveys [ZZLH19, WLLZ20].\nText-based Animation Creation. Text-based animation offers dy-\nnamic illustration of text-form storytellings. Irene et al. [AHK\n\xe2\x88\x9702]\nproposed a rule-based method to convert text into an animation se-quence of facial expressions, which is synchronized with speech.\nHayashi et al. [HID\n\xe2\x88\x9714] proposed a text-to-CG animation genera-\ntion method based on TV program Making Language (TVML), but\nthe generated characters only perform pre-de\xef\xac\x81ned sets of actions at\xef\xac\x81xed positions. Abrami et al. [AHKM20] proposed a text-to-scenemethod that allows users to interactively build a virtual scene dur-ing immersive exploration via VR headset and controllers. Ghoshet al. [GCO\n\xe2\x88\x9721] proposed a text-driven skeleton animation method\nbased on deep neural networks, which can handle short sentencescomposed by a single action or long sentences composed by a com-bination of multiple consecutive actions. However, their methodmay fail to generate continuous actions that are not adequately dis-\ntributed in the dataset.\nGeneration of 3D Character Motion. Mocap solutions are usually\nused to customize realistic character animation in industry. It is la-\nborious and time-consuming to deploy mocap apparatuses and con-\nduct the experiments. It is always expected to simplify or even auto-\nmate the process [RLA\n\xe2\x88\x9718,LZL20]. Dietmar et al. [SPH11] simul-\ntaneously generated speech and CG animation based on the Hidden\nMarkov Model. Kapadia et al. [KFS\xe2\x88\x9716] proposed an optimization\nalgorithm to automatically \xef\xac\x81ll in missing narration details for given\nkey plot points in a story to synthesize a complete 3D animation.\n\xc2\xa9 2021 The Author(s)\nComputer Graphics Forum \xc2\xa9 2021 The Eurographics Association and John Wiley & Sons Ltd.218'
