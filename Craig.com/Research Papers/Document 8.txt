b' Towards Context-Aware Face Recognition \nMarc Davis1, Michael Smith2, John Canny3, Nathan  Good1, Simon King1, Rajkumar Janakiraman4 \nSchool of Information Management and Systems, U.C. Berkeley, {marc, ngood, simonpk}@sims.berkeley.edu1 \nFrance Telecom R&D, South San Francisco, CA, michael.smith@rd.francetelecom.com2 \nComputer Science Division, U.C. Berkeley, Berkeley, CA, jfc@cs.berkeley.edu3  \nSchool of Computing, National University Singapore, Singapore, janakira@comp.nus.edu.sg4\nABSTRACT  \nIn this paper, we focus on the use of context-aware, collaborative filtering, machine-learning techniques that leverage automatically sensed and inferred contextual metadata together with computer vision analysis of image content to make accurate predictions about the human subjects depicted in cameraphone photos. We apply Sparse-Factor Analysis (SFA) to both the contextual metadata gathered in the MMM2 system and the results of PCA (Principal Components Analysis) of the photo content to achieve a 60% f ace \nrecognition accuracy of people depicted in our camera phone photos, \nwhich is 40% better than media analysis alone. In short, we use context-aware media analysis to solve the face recognition problem for cameraphone photos. \nCategories and Subject Descriptors  \nH.5.1 [ Information Interfaces and Presentation ( e.g., HCI) ]: \nMultimedia Information Systems; H.3.1 [ Information Storage and  \nRetrieval ]: Content  Analysis  and  Indexing;  H.3.3  [Information \nStorage and Retrieval ]: Information Search and Retrieval; I.4.8 \n[Image  Processing and Computer Vision ]: Scene Analysis.   \nGeneral Terms: Algorithms, Design, Experimentation. \nKeywords: Context-Aware, Face Recognition, Camera phone, \nSFA, PCA \n1. INTRODUCTION \nCameraphones are becoming the dominant platform for digital \nimaging worldwide and have continued to surpass sales of digital cameras since the first half of 2003. InfoTrends/CAP Ventures predicts that 860 million cameraphones will be shipped in 2009, accounting for 89% of all mobile phone handsets.  Most important for multimedia researchers, in addition to their growing global ubiquity, cameraphones offer a unique opportunity to purse new approaches to media analysis and management: namely to combine the analysis of automatically gathered contextual metadata with media content analysis to radically improve image content recognition and retrieval. The fundamental challenges of media content analysis have been understood for several years [7]\xe2\x80\x94using contextual metadata gathered from groups of users capturing and sharing media on cameraphones we can \xe2\x80\x9creclaim the world\xe2\x80\x99 by adding context  and memory  to multimedia processing and retrieval \n[3, 4].  In short, we are bridging the semantic and sensory gap by reconnecting multimedia analysis to the context in which the media \nwas captured and to the patterns of media capture and use of individual users and groups of users.  \nWe capture a plethora of contextual metadata using the sensors \navailable on cameraphones: temporal (exact time served from the cellular network); spatial (Cell ID from the cellular network and location from Bluetooth-connected GPS receivers); and social (who took the photo, who sent and/or received the photo when shared, and who was co-present when the photo was taken sensed via Bluetooth MAC addresses mapped to usernames). Using contextual metadata together with media analysis, we have employed collaborative filtering machine learning techniques to predict the probability that a given user has photographed a given subject (e.g., person or place) in a given spatio-temporal-social context. We achieve 60% face recognition accuracy of people depicted in our cameraphone photos. This result represents a nearly 40% improvement over PCA alone (the best performing publicly available face recognition algorithm) which has a 43% face recognition accuracy on the same dataset. \n1.1 Face Recognition on Cameraphone Data \nFace recognition is highly dependent on frontal pose and not well suited for cameraphone photos. The quotidian use and portability of a cameraphone leads to a capture environment that is often more varied than that of photos of human subjects captured with conventional cameras. Cameraphone users often take spontaneous photos [9] often with non-frontal subjects, as shown in the bottom row of Figure 1. The low resolution and slow shutter speed of current cameraphones, which creates motion blur, or grainy photos in poor lighting conditions, also reduces face recognition accuracy. The much higher accuracy of the same vision algorithms we use in our study in face recognition trials using the NIST FERET dataset (http://www.frvt.org/FERET/default.htm) may be attributed to the \xe2\x80\x9cmug shot\xe2\x80\x9d quality of the photos in the NIST FERET corpus, i.e., each photo is of people depicted in full frontal view in a head-and-\nshoulders shot. Our MMM2 corpus of over 25,000 cameraphone photos collected by our 66 users over 9 months shows much greater variability of photo conditions and often has multiple people depicted per photo\xe2\x80\x94as such, our study attempts to test the \xe2\x80\x9creal world\xe2\x80\x9d accuracy of face recognition algorithms and approaches.  \n2. RELATED WORK \nThe research of Naaman, et al., uses similar context features as our work for identifying human subjects [5]. With the exception of features for weekend vs. weekday photo capture, and indoor vs. outdoor capture, they also use event, location, time, and neighboring information. The key differentiator is that we combine contextual analysis with signal-based face recognition to pr oduce a better result \nthan contextual analysis or computer vision alone can provide. Other research cited in [5] has explored methods for face image annotation that focus on image similarity, thumbnail visualization, and intuitive interfaces. Much of this work is focused on a nnotation  \nP e r m i s s i o n  t o  m a k e  d i g i t a l  o r  h a r d  c o p i e s  o f  a l l  o r  p a r t  o f  t h i s  w o r k  f o r  \npersonal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. MM\xe2\x80\x9905 , November 6\xe2\x80\x9311, 2005, Singapore. \nCopyright 2005 ACM 1-59593-044-2/05/0011\xe2\x80\xa6$5.00. \n \n483 \n'
b'interfaces. In prior work, a list of candidates is presented for \nverification using a compact interface. New met hods in f ace \nrecognition, such as high-resolution images, three-dimensional face recognition, and new preprocessing techniques may offer improved accuracy [6], but our context-aware approach utilizes comparatively lightweight computation and offers significantly improved performance today. \n3. SYSTEM OVERVIEW  \n3.1 MMM2: Gathering Data and Metadata \nThe Mobile Media Metadata 2 (MMM2) system consists of two \nprimary components: the Context Logger, running on the cameraphone, and the server application running on a Linux server [2]. The Context Logger is responsible for capturing contextual metadata and uploading photos and metadata to the server. The server application manages photos, their associated metadata, and user profile information. The server application uses a set of servlets and Java Server Pages to generate customized HTML for display on a PC-based web browser or the Opera web browser on a cameraphone handset. \n3.1.1 MMM2 Context Logger \nThe Context Logger (developed by and modified in cooperation with the University of Helsinki Department of Computer Science Context Project (http://www.cs.helsinki.fi/gr oup/context/) is a Nokia \nSeries 60 application that runs as a background process and continually captures contextual metadata and monitors the phone\xe2\x80\x99s file system for newly created media (photos, videos, and audio clips). The Context Logger records most phone actions\xe2\x80\x94when a voice call is received or initiated, when the phone switches to a new \ncell tower, when the phone is charged.  Additionally the logger also uses the phone\xe2\x80\x99s Bluetooth device to periodically poll for the presence of other nearby Bluetooth-enabled devices. The Context Logger can also communicate with a Bluetooth-enabled GPS devices (we use the HP iPAQ Bluetooth GPS Navigation System) to record GPS location information. When the Context Logger detects a new media file on the file system it displays a one-screen user interface and begins to upload the media file over HTTP to the server. Appended to the end of the image data is an XML fragment containing the context snapshot. This snapshot includes the time, current cell ID, sensed Bluetooth devices, and GPS location information (if available). \n3.2 Creation of Ground-Truth Dataset \nTo create a set of photos annotated with labeled f aces we used a \ncustom-built Java applet that can be accessed on the web, linked from the MMM2 website. The applet allows a user to select a region of a photo and associate a person\xe2\x80\x99s name with this region. Selecting a region associated with each face rather than simply a single point with the face allows this metadata to be used for face detection as well as recognition. Users were instructed to select regions of the photo containing f aces (from ear to ear, forehead to chin), which \nwere at least 20-30 pixels wide and in which the face is visible enough for the human annotator to recognize it. In an effort to create a dense set of annotated photos in which many f aces appear many \ntimes, rather than a sparse set in which many faces appear only a few times, we had several MMM2 users (primarily from the development team) use this annotation tool to annotate as many photos as possible.   Close Frontal Pose \n  \n   \n  \nDistorted Pose \n  \n   \n  \nFigure 1. (Top) Subjects with frontal pose, (Bottom) Same \nsubjects with non-frontal or distorted pose. \nEleven users total used the annotation tool, seven of which each \nannotated at least 20 photos. The result is a dataset of 1057 photos with faces, covering 173 different f aces with 31 faces occurring at \nleast 10 times each and 58 faces appearing at least 5 times each. While only 1057 photos had f aces, the a nnotation process also \nproduced a set of nearly 2000 additional photos known not to contain faces. While these additional photos are of no use in \nmachine vision face recognition, the data can still be used in attempting to determine the contexts in which a user is likely to be photographing people rather than non-person subjects. Examples of the photos taken are shown in Figure 1. Frontal pose images represent a small fraction of our face images. \n3.3 Content Analysis: Face Recognition  \nFace recognition has long been the standard for identifying humans in images. Current methods attempt to detect key facial features such as eyes, nose, and lips, and match these features to known templates for human faces. Evaluation of these met hods usually occurs with \nfrontal facing images, such as those shown in the top row of Figure 1. Problems occur when facial imagery is not frontal. Most of the images in this research were taken in natural settings with limited frontal pose, as shown in the bottom row of Figure 1. We tested 4 publicly available face recognition                                      systems implemented by Colorado State University (CSU - http://www.cs.colostate.edu/evalfacerec/): \n\xe2\x80\xa2 PCA: Eigenfaces principle com ponents analysis based on linear \ntransformations in feature space. PCA requires a short training time and uses a relatively small dimensionality of feature vectors. Many distance measures can be used, but we received the best accuracy with Euclidean and Mahalinobis. \n\xe2\x80\xa2 LDA+PCA Combination: Linear discriminant analysis based on \nthe University of Maryland algorithm in the FERET tests. LDA training requires multiple images and first using PCA to reduce the dimensionality of the feature vectors.  \n\xe2\x80\xa2 Bayesian MAP: Maximum a posteriori (MAP) difference \nclassifier based on the MIT algorithm in the FERET tests.  \n\xe2\x80\xa2 Bayesian ML: Maximum likelihood (ML) classifier based on the \nsame MIT Algorithm above.  \n3.4 Predicting Faces Using SFA \nTo combine a diverse set of data and metadata for face identity \nprediction, we used a general-purpose inference algorithm called Sparse-Factor Analysis (SFA). SFA is a linear probabilistic model  \n \n484 '
